# General Linear Models {#glm}

```{r setup-glm, include=FALSE}
knitr::opts_chunk$set(out.width="50%", out.height="50%", fig.align='center',
                      cache=TRUE,
                      warning=FALSE, error=FALSE)
```

General linear models are a key member of the generalised linear modelling family and they are among the most widely used and reported models in the marine science literature, particularly biology. This course focuses on two subsets of linear models: ANOVA and regression. ANOVA and regression are typically used for different data modelling scenarios: ANOVA when you’ve got a categorical predictor variable(s) and regression when your predictor is continuous. 


## Analysis of variance (ANOVA)

ANOVA is a widely used modelling approach that enables you to compare means and put confidence intervals on the differences between those means. For a predictor with only two categories, ANOVA is identical to the 2-sample t-test, so we'll just use ANOVA. 

When you see "analysis of variance", think "analysis of means". In an ANOVA, we analyse the variance in the data in order to compare the means of different groups. In an ANOVA, we compare means by determining the ratio of [the variance between treatments and the overall mean (large black arrows in Fig. \@ref(fig:var-sources))] and [the variance within treatments (sum of the dotted arrows in Fig. \@ref(fig:var-sources))]. The black line under the red dots in Fig. \@ref(fig:var-sources) shows the actual data distribution and the actual parameters for mean values (50 and 150 cm for A and B respectively). You then take samples from A and B (n=4 in this example) and, from these, derive your parameter estimates for the mean of each group and the overall mean.

In Fig. \@ref(fig:var-sources) below you can see that the solid black arrows are much larger than the dotted ones leading you to think that the chance that these two samples are drawn from the same population (with a value of the overall mean) as very unlikely. Make sure you understand Fig. \@ref(fig:var-sources) (more detail in the ANOVA lecture).

In most circumstances you know that the means that you are comparing with ANOVA are different (i.e. that testing a null hypothesis of no difference isn’t useful). ANOVA allows you to put confidence intervals around differences between means, or groups of means.

\begin{figure}
\centerline{\includegraphics[width=0.6\textwidth]{figs/ANOVA_sources_of_var.png}}
\caption{Sources of variation (within group and between groups) as quantified by ANOVA.}
\label{fig:var-sources}
\end{figure}


### ANOVA in R

There are numerous variations on the theme of ANOVA. We cover one-way ANOVA and we mention two-way ANOVA with and without replication. The objective of ANOVA is to establish the size of the difference (called the ‘effect size’) between different groups (e.g., treatments or locations) and put a confidence interval on those differences. 



### One-way ANOVA

One-way ANOVA is a procedure we use to estimate the magnitude of differences between means of $\geq$ 2 groups. We also use it to put confidence intervals on those differences. 

The first example data is the yield in $\mu g$ C $ml^{-1}$ of a species of microalgae (*Isochrysis galbana*) in laboratory culture exposed to three light levels (low, medium and high). We are interested in these particular light levels because they represent the means of winter, spring, and summer Scottish sun intensity. The data are in worksheet ‘Microalgae’.

Note: the function to conduct an ANOVA is `aov()`. The function `anova()` converts various statistical model outputs to the standard ANOVA-table output (including any from the GLM family). 


```{r}
library(readxl)
algae_wide_df <- read_excel("data/practical_5.xlsx", sheet = "Microalgae")
# check these data as usual
```


*Q158. What is your objective in this type of experiment? What are you interested in estimating?*

*Q159. What assumptions should be met prior to undertaking parametric ANOVA?*

*Q160. Under which circumstances could you begin to relax the assumption that the data are normally distributed (think central limit theorem)?*

*Q161. Given your sample size can we assume normality of means?*

*Q162. Are the data normally distributed (be careful how you word your answer to this question, see the following question)?*

*Q163. Is it reasonable to assume that these data are drawn from a population that is normally distributed?*

We can check the homoscedasticity assumption using Bartlett’s test. Before we can use this test we need to rearrange the data so that the data is in a single indexed column. We'll use the *tidyverse* as before.


```{r, message=F}
library(tidyverse)
head(algae_wide_df, 2)
algae_df <- algae_wide_df |>
  pivot_longer(everything(), names_to = "Treatment", values_to = "Yield")
glimpse(algae_df, width=80)
bartlett.test(Yield ~ Treatment, data = algae_df) #?bartlett.test
```


*Q164. With regard to Bartlett's test, is the assumption of homogeneity reasonable?*

A more elegant (and much better) way of checking model assumptions is to check residual patterns. A ‘residual’ is the difference between an actual data value and that predicted by the model. Here we have randomly assigned five cultures each of the same species to three specific treatments (light levels).

*Q165. What type of experiment is this (how many factors, are they fixed or random)?*

Next to conduct the analysis:

```{r}
algae_aov <- aov(Yield ~ Treatment, data = algae_df) # ?aov
```


Before we look at the output, let's assess the assumptions using the residuals. The default residual plots created by R are shown in Fig. \@ref(fig:algae-aov) and enables us to rapidly assess whether the model assumptions are reasonable. 

```{r algae-aov, fig.cap="Residual plots from one-way ANOVA.", fig.dim=c(5.5,5)}
par(mfrow = c(2, 2), mar=c(4,4,1,1)) 
plot(algae_aov) 
```

**Interpretation of residual patterns:**

- **Upper left:** Residuals v. fitted. This is the residual values against the fitted values. The fitted values are the means of the three groups (remember that ANOVA is about comparing means). The spread for the lower values (low and medium light) is higher than for the high light so this might make us consider the homoscedasticity assumption.  
- **Upper right:** Normal Q-Q plot. This assesses the normality assumption. The points (each point is an observation) lie around the straight line so this assumption is reasonable. Note that general linear models assume that the means of groups are normally distributed, and this always applies when the means are based on large sample sizes (roughly $n > 30$)). When $n < 30$, you should check that the distribution of the residuals is reasonably ‘normal’.  
- **Lower left:** Scale-location. This specifically looks to assess whether residuals increase with fitted values, which is a common issue in these types of analysis. In this case, the scale decreases with fitted value. This is similar to the Upper Left plot, but with `sqrt(abs(standardized_residuals))` on the y-axis instead of just `residuals` to focus just on the magnitude of the residuals.  
- **Lower right:** Constant leverage, residuals vs. factor levels. This indicates how each treatment is fitted (i.e. the residuals associated with each treatment). You might be concerned if one particular treatment was associated with extremely high residuals (outliers). R automatically identifies potential outliers (8, 11, and 13 in this case) for you to further assess. In this case there is nothing in particular to worry about.

The residual plots allow you to investigate different aspects of the data and the how their assumptions are met. The interpretation of the plots overlaps in the sense that the same issue might be apparent in several of the plots.

*Q166. What are the ‘fitted values’ for an ANOVA?*

*Q167. Are your effects fixed or random?*

*Q168. Assuming you have chosen* $\alpha = 0.05$*, what might you be interested in going on to test next? Hint: you are testing to see whether the means of three populations are different.*

Everything looks ok, so we can then look at the results of the ANOVA.

```{r}
# anova(algae_aov) # outputs an anova-type table, but unnecessary with aov()
summary(algae_aov)
```

Reporting that there are ‘significant’ differences between means is not enough. What your readers should be interested in is what the differences between the means actually are, and how confident you are in your assessment. This can be provided by the Tukey test in R.

```{r}
algae_grp_diffs <- TukeyHSD(x = algae_aov, conf.level = 0.95) 
# HSD stands for 'honestly significant difference'
algae_grp_diffs
```

You can see that the mean yield at the high light level is higher than at both the low and medium: the 95% confidence interval of the difference in comparing high and low light levels are 1.69 (0.205, 3.17) $\mu g$ C $ml^{-1}$. (I’ve inverted the results so the the difference is seen as positive (high - low rather than low - high)). This confidence interval is much more important than any P-values and you should report both (CI because it is useful, P value by convention).


### Two-way ANOVA

You’ve had a look at one-way ANOVA (i.e., one predictor), which is a good starting point. However, in nature you often find numerous factors combine to influence an outcome. This is called an ‘interaction’. Two-way ANOVA allows you to investigate the nature of this interaction term. You can also get three-way ANOVA and more, but these get logistically challenging because you need to replicate across each level. The interpretation also gets increasingly difficult. You need to be aware of the existence of two-way ANOVA and what it offers, and how to interpret simple graphics (below), but we do not cover implementation of two-way ANOVA.

\begin{figure}
\centerline{\includegraphics[width=0.6\textwidth]{figs/interactions.png}}
\caption{Graphic illustrating differing levels of interaction on the response from two treatments A and B.}
\label{fig:interaction}
\end{figure}


In Fig. \@ref(fig:interaction) we have the outcome of an experiment. Each dot on the plot represents the mean Response (e.g. growth) of a number of replicates, subject to the combination of Temperature (cold and hot) and Nutrient (N and P). We are interested in the main effects (Temp and Nutrient) and their interaction (Temp * Nutrient) In panel A, there is no effect of Temp on the Response (the lines between Cold and Hot are horizontal), but there is a main effect of Nutrient (P is higher than N). In B, there is an effect of Temp (Hot is, on average higher than Cold) but there is also an interaction as the effect of Hot is more for P than for N (where it has no effect in this example). In C, the interaction is stronger compared with B. In D, there are no treatment main effects because mean Cold = mean Hot and mean N = mean P, but there is a very strong interaction effect; the effect of Temp is reversed by Nutrient, so that the level of Nutrient (N or P) determines the effect of Temp. When it is Cold the Response is high for Nutrient P and low for Nutrient N, when it is Hot, the Response is low for Nutrient P and high for Nutrient N (but the average for hot=cold, and average for N=P)


---------------------------------------------------


## Regression

Correlation and regression are used to examine the strength of association between two variables. In correlation, both variables are measured (and therefore associated with measurement error). In regression, one variable is fixed (by the experimenter) and is asssumed to have no ‘error’ associated with it and the other, called the ‘response variable’, is measured (so has measurement error). You must be able to distinguish which of correlation or regression analyses are most appropriate. 

Correlation analysis is used to measure association, where you are not attempting to formally link cause-and-effect. Regression analysis is generally used where you have experimentally manipulated the fixed factor and are looking at the response in another factor. Causation is implicit in inferential regression analysis (correlation analysis is often used in ‘exploratory’ data analysis where any link between cause-and-effect is inherently more speculative). 

The media often misreport science because it is difficult to resist the impulse to attribute causation. An overwhelming number of spurious correlations (i.e., those *clearly* having no causal relationship) are documented on [tylervigen.com](https://tylervigen.com/spurious-correlations).


### Overview

Regression is at the heart of linear models. ANOVA and t-tests are, basically, special cases of linear regression models. The regression coefficient is a measure of the strength of the relationship between the dependent variable (the one you measure) and the independent variable (the one you fix like a fixed factor in ANOVA). The regression coefficient is denoted by $R^2$ compared with $r$ in correlation. The regression coefficient $R^2$ ranges from 0 to 1 (unlike $r$ which ranges from -1 to 1). A value $R^2 = 0$ indicates no relationship to the independent variable while $R^2=1$ indicates that the independent variable is entirely responsible for the variability in the measured variable.

As usual, null hypothesis significance testing is often applied to regression statistics. As usual, the null hypothesis being tested is usually “there is no functional relationship between the response and the predictor” and this is usually conceptually nonsense. In conducting regression analysis, your objective is to quantify to the most appropriate precision and accuracy possible the relationship between X (the aspect you control, the predictor, plotted on the X axis) and Y (the variable you measure, the response, plotted on the Y axis). Your objective is to quanitfy this relationship, put confidence intervals on it, and then interpret your findings in relation to the objectives of the study and in relation to other research.

*Q169. Sketch a graph demonstrating the null hypothesis (of no relationship) in regression analysis.*
\newline
\newline
\newline
\newline
\newline

Let us now consider an example in which cause and effect does exist. The data in worksheet ‘Regression1’ shows the weight loss in *Tribolium confusum*, the confused flour beetle, at different relative humidities (data from Sokal and Rohlf, 1995). The relative humidity (RH) to which the beetles are exposed can be fixed and the weight loss (via evaporative losses) of the beetles then assessed. There is no way that the null hypothesis can be true in this case: humidity will obviously influence weight loss in beetles.

*Q170. In this case, what is your response variable (what are you measuring) and your predictor (i.e. what is it that you are manipulating to determine the extent of the response)?*

*Q171. Plot the data in R and check your prediction. In this case, the predictor must be displayed on the x-axis and the response must be on the y-axis.*

We are interested in whether the whole data set can be usefully represented by a linear regression relationship. We wish to estimate the relationship, and put a confidence interval on our estimate. Common sense tells us that there *is* some sort of relationship (testing a null hypothesis is not very useful) but it might go in either direction (positive or negative) and we don’t know the strength (i.e. slope) of that relationship.


### Linear regression in R

In R we can use a variety of techniques to conduct linear regression. The easiest is to use `lm()`. It is worth noting that `lm()` would also work for all your other general linear models (e.g. ANOVA). They are, in fact, the same model, it is just the default output (and necessary input formatting) that differs. Try reproducing the ANOVAs above with `anova(lm(...))`.


Import data and begin:

```{r beetle-scatter, fig.dim=c(4,4), out.width="40%"}
beetle_df <- read_excel("data/practical_6.xlsx", sheet = "Beetles")
# inspect the dataframe, then make a scatter plot
par(mfrow=c(1,1))
plot(WeightLoss_Mg ~ Humidity, data = beetle_df) 
```

An aside on plotting: you can provide `plot()` with either a vector for the x-axis and a vector for the y-axis (i.e., `plot(x_var, y_var)`) *or* you can use a formula, specifying the dataframe (i.e., `plot(y ~ x, data=data_df)`). Just be aware of which variable is on which axis.

Now we have explored and plotted the data we can conduct the regression analysis.

```{r}
# weightloss is modelled as (~) a function of humidity
beetle_lm <- lm(WeightLoss_Mg ~ Humidity, data = beetle_df) 
# beetle_lm
# str(beetle_lm) # lm outputs are complex structures
```


Before we go on and interpret the model output we need to assess the model assumptions. This is done in the same way as for ANOVA with the same commands.

```{r beetle-diag, fig.dim=c(5,5)}
par(mfrow = c(2, 2), mar=c(4,4,1,1)) # set up 4 in 1 plot.
plot(beetle_lm) # plot the regresssion residuals.
```



The small sample size here ($n=9$) makes a proper analysis of the residuals difficult. The plot should be assessed in the same way as for the ANOVA residuals. Basically, any pattern is bad. The upper left (Residuals v Fitted) doesn’t cause any major concern, thought upper right (Normal QQ) indicates a possible problem. Scale-Location (lower left) is difficult to interpret but no obvious pattern is present. The Residuals v. leverage (lower right) indicates a potential issue as well. A point with a large residual (i.e. where it is very different to that expected by the model) and with a high leverage (i.e. at the extreme ends of the predictors range) has a large Cook’s distance and has a disproportionate effect on the slope and intercept. These points should be examined in more detail.

*Q172. Which point has the largest Cook’s distance?*

We will now proceed to looking at the linear regression analysis results on the basis that the residuals do not raise any concerns.


```{r}
summary(beetle_lm)
```


The regression equation of the form $y = a + bx$ can be determined. The regression equation is:

$WeightLoss = 8.70 - 0.05322 * humidity$

Common-sense check: the coefficient is negative. As the humidity increases, the weight loss decreases (as expected and shown in the scatter plot).

*Q173. What is the effect on weight loss of increasing the relative humidity by 10%?*

*Q174. What is the weight loss, predicted by the model, when relative humidity is 0%?*

*Q175. What does the model suggest the weight loss will be when relative humidity is -50% and +150%? Are these values sensible? What does this tell you about extrapolating beyond the data range in using regression analysis in predictions?*

The residual error is the variance in y around the line. The $R^2$ is the proportion of this variance that is explained by the regression line. In the current case $R^2 = 0.97$. This is an extremely high value and indicates that the regression model is extraordinarily good at accounting for the variance in weight loss based on the relative humidity.

The P values allow us to assess if the slope and the intercept are likely different from zero.

*Q176. Given the very high* $R^2$ *(and looking at your plot) would you expect the regression model to be significantly better than the null model in explaining the variance in weight loss?*

*Q177. With* $\alpha = 0.05$*, do you reject or accept the null hypothesis? What would you wish to report in relation to the slope coefficient if you were reporting the results from this analysis?*


```{r}
confint(beetle_lm)
```

The confidence intervals are, again, ‘clunky’ to describe. What the above table indicates is that, if alternate yous (like in a multiverse) repeated your experiment with the same sample size, you would expect the intercept of the line to lie between 8.25 and 9.16 (3 sf) in 95% of those replicates, and you would expect the slope to vary between -0.0455 and -0.0609. The confidence interval is the scientifically interesting bit rather than the P value. 


### Plotting the regression line and confidence intervals

A regression model (i.e. the linear relationship between the predictor and response variables) allows us to predict values for any value of the predictor, along with confidence levels. We can plot this regression line without too much effort.


```{r beetle-regline, fig.cap="Regression line (solid) with upper and lower 95\\% CI (dashed)", fig.dim=c(4,4)}
beetle_pred <- predict(beetle_lm, interval = "confidence", level = 0.95)

par(mfrow=c(1,1))
plot(WeightLoss_Mg ~ Humidity, data = beetle_df, 
     xlab = "Relative humidity (%)", ylab = "Weight loss (mg)") 
lines(beetle_df$Humidity, beetle_pred[, "fit"])
lines(beetle_df$Humidity, beetle_pred[, "lwr"], lty = 2)
lines(beetle_df$Humidity, beetle_pred[, "upr"], lty = 2)
```

Try generating 90% confidence intervals and add them to the plot. 

*Q178. Which will have the wider interval, a 99.99% interval or a 50% interval and why?*

*Q179. Do the confidence intervals in Fig. \@ref(fig:beetle-regline) run parallel to the regression line?*

*Q180. If not, what does this suggest about the degree of confidence you have in values predicted at various points along the line?*

*Q181. At what value of relative humidity are your predictions of weight loss likely most accurate?*

We can make predictions based on our regression line, and put confidence intervals on those predictions. Say we had a relative humidity of 50% in the above example. You could ask for the model-predicted weight loss and you’d want confidence intervals on that prediction.


```{r}
# predict() needs a data.frame with the same predictors used in beetle_lm
predict(beetle_lm, 
        newdata = data.frame(Humidity = 50), 
        interval = "predict", 
        level = 0.95)
# or more fully:
predict(beetle_lm, 
        newdata = data.frame(Humidity = seq(0, 100, by=25)), 
        interval = "predict", 
        level = 0.95)
```


These are prediction intervals and they are broader than confidence intervals. The confidence intervals express your confidence about where the mean regression line would lie (if you went back in time and were somehow able to repeatedly sample from the same population). The prediction interval expresses your confidence about where the *values* would lie if you went back and repeatedly took an individual of the population at a given value of the predictor (e.g. 50% humidity).
For more kicks, import PhosphateCalibration (1st year practical data) into R and duplicate the following plots and confidence intervals.


```{r}
phosphate_df <- read_xlsx("data/Practical_6.xlsx", "PhosphateCalibration")
```

```{r echo=FALSE, fig.dim=c(4,4)}
par(mfrow=c(1,1))
plot(Absorbance ~ Concentration, data = phosphate_df)
```

```{r echo=FALSE, fig.dim=c(5,5)}
par(mfrow=c(2,2), mar=c(4,4,1,1))
phos.mod <- lm(Absorbance ~ Concentration, data = phosphate_df)
plot(phos.mod)
```

```{r echo=FALSE}
summary(phos.mod)
```

*Q182. Are you happy with your model assumptions?*

*Q183. Write down the regression equation.*

*Q184. Determine the confidence interval for the regression line.*

*Q185. For a concentration of 0.75 units, what values would you expect (95 times in 100) to see from your experimental set-up?*

You should get:

```{r echo=FALSE}
predict(phos.mod, newdata=data.frame(Concentration = 0.75), interval = "predict")
```



--------------------------------------------------------------


## Conclusions

Correlation is a measure of association between two variables. It is appropriate to use correlation to measure this association when one cannot or does not wish to assume that any relationship is causative. Pearson correlation coefficients should only be used where it is fair to assume (by looking at scatter plot) that the relationship is approximatley linear. Where linearity does not apply, attempt to transform one or both of the variables. Where there are outliers (that cannot be removed) or where one is uncertain about some of the data, then non-parametric ranked based correlation coefficients, such as the Spearman coefficient, should be used. As with GLMs, correlation analysis assumes that all points are independent of each other.

Linear regression is one of the most widely used statistical techniques. It is used to examine causal relationships, often where experimental manipulations are conducted. Regression is a general linear model and it lies within the generalised linear model family (GLMs). GLMs allow you to model data that is not normally distributed, including proportions (bounded by 0 and 1), or counts (bounded by 0). Using a GLM is a much better way of analysing these data compared with transforming the response variable or using non-parametric techniques. All members of the GLM family make the assumptions that measurements are independent of each other. Where this assumption fails you can use generalised linear mixed models (GLMMs). Extensions of simple linear regression include multiple regression which examines the influence of two or more continuous variables on a response variable.
